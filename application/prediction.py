# -*- coding: utf-8 -*-
"""FraminghamHeartDiseasePrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nZa4jDnBrOBanf7MilnDl13mTFQeBu3G

# Import Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler
from sklearn.utils import resample
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.compose import make_column_transformer
from sklearn.feature_selection import SelectKBest, chi2

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.naive_bayes import GaussianNB
import xgboost as xgb
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score, roc_curve, roc_auc_score, cohen_kappa_score

from imblearn.under_sampling import RandomUnderSampler
from imblearn.ensemble import BalancedBaggingClassifier

from imblearn.over_sampling import SMOTE

from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"
# %config InlineBackend.figure_format ='retina'
# %matplotlib inline

"""# Exploratory Data Analysis

Data reading and displaying first 5 rows of the data
"""

df = pd.read_csv("https://raw.githubusercontent.com/theleadio/datascience_demo/master/framingham.csv")
df.head()

"""Numerical properties for each column of the dataset: number of values, mean, standard deviation, minimum, maximum, 25th, 50th and 75th percentiles

"""

df.describe().T.style.set_properties(**{'background-color': 'grey','color': 'white','border-color': 'white'})

"""Description of the dataset: size, datatype of each variable and number of non null values for each variable

"""

df.info()

"""Number of unique values for each variable - Note some are very high as they are continuous pieces of data

"""

df.nunique()

"""Removal of all rows in dataset with one or more null value and then description of datset to ensure no null values remain

"""

df.dropna(axis=0,inplace=True)
df.info()

"""Display all duplicate rows in the dataset"""

duplicate_df = df[df.duplicated()]
duplicate_df

"""Display histogram for all variables of the dataset"""

fig = plt.figure(figsize = (15,20))
ax = fig.gca()
df.hist(ax = ax)

"""Heatmap to show correlation between all variables in the dataset"""

sns.set(rc={'figure.figsize':(20,15)})
sns.heatmap(df.corr(), annot=True)

fig, ax = plt.subplots(figsize = (13,5))
sns.kdeplot(df[df["TenYearCHD"]==1]["age"], alpha=0.5,shade = True, color="red", label="TenYearCHD", ax = ax)
sns.kdeplot(df[df["TenYearCHD"]==0]["age"], alpha=0.5,shade = True, color="green", label="Normal", ax = ax)
plt.title('Distribution of Age', fontsize = 18)
ax.set_xlabel("Age")
ax.set_ylabel("Frequency")
ax.legend();
plt.show()

fig, ax = plt.subplots(figsize = (13,5))
sns.kdeplot(df[df["TenYearCHD"]==1]["totChol"], alpha=0.5,shade = True, color="red", label="TenYearCHD", ax = ax)
sns.kdeplot(df[df["TenYearCHD"]==0]["totChol"], alpha=0.5,shade = True, color="green", label="Normal", ax = ax)
plt.title('Distribution of Total Cholestrol', fontsize = 18)
ax.set_xlabel("Total Cholestrol")
ax.set_ylabel("Frequency")
ax.legend();
plt.show()

fig, ax = plt.subplots(figsize = (13,5))
sns.kdeplot(df[df["TenYearCHD"]==1]["sysBP"], alpha=0.5,shade = True, color="red", label="TenYearCHD", ax = ax)
sns.kdeplot(df[df["TenYearCHD"]==0]["sysBP"], alpha=0.5,shade = True, color="green", label="Normal", ax = ax)
plt.title('Distribution of Systolic Blood Pressure', fontsize = 18)
ax.set_xlabel("Systolic Blood Pressure")
ax.set_ylabel("Frequency")
ax.legend();
plt.show()

fig, ax = plt.subplots(figsize = (13,5))
sns.kdeplot(df[df["TenYearCHD"]==1]["diaBP"], alpha=0.5,shade = True, color="red", label="TenYearCHD", ax = ax)
sns.kdeplot(df[df["TenYearCHD"]==0]["diaBP"], alpha=0.5,shade = True, color="green", label="Normal", ax = ax)
plt.title('Distribution of Diastolic Blood Pressure', fontsize = 18)
ax.set_xlabel("Systolic Blood Pressure")
ax.set_ylabel("Frequency")
ax.legend();
plt.show()

fig, ax = plt.subplots(figsize = (13,5))
sns.kdeplot(df[df["TenYearCHD"]==1]["BMI"], alpha=0.5,shade = True, color="red", label="TenYearCHD", ax = ax)
sns.kdeplot(df[df["TenYearCHD"]==0]["BMI"], alpha=0.5,shade = True, color="green", label="Normal", ax = ax)
plt.title('Distribution of BMI', fontsize = 18)
ax.set_xlabel("BMI")
ax.set_ylabel("Frequency")
ax.legend();
plt.show()

fig, ax = plt.subplots(figsize = (13,5))
sns.kdeplot(df[df["TenYearCHD"]==1]["heartRate"], alpha=0.5,shade = True, color="red", label="TenYearCHD", ax = ax)
sns.kdeplot(df[df["TenYearCHD"]==0]["heartRate"], alpha=0.5,shade = True, color="green", label="Normal", ax = ax)
plt.title('Distribution of Heart Rate', fontsize = 18)
ax.set_xlabel("Heart Rate")
ax.set_ylabel("Frequency")
ax.legend();
plt.show()

fig, ax = plt.subplots(figsize = (13,5))
sns.kdeplot(df[df["TenYearCHD"]==1]["glucose"], alpha=0.5,shade = True, color="red", label="TenYearCHD", ax = ax)
sns.kdeplot(df[df["TenYearCHD"]==0]["glucose"], alpha=0.5,shade = True, color="green", label="Normal", ax = ax)
plt.title('Distribution of Glucose', fontsize = 18)
ax.set_xlabel("Glucose")
ax.set_ylabel("Frequency")
ax.legend();
plt.show()

"""# Preprocessing

Balancing
"""

X = df.drop(columns =['TenYearCHD'], axis = 1)
y = df['TenYearCHD']

y.value_counts().plot.pie(autopct='%.2f', figsize=[5,5])

healthy_df = df[(df['TenYearCHD']==0)]
unhealthy_df = df[(df['TenYearCHD']==1)]

sm = SMOTE(sampling_strategy='minority', random_state=42)
oversampled_X, oversampled_Y = sm.fit_resample(df.drop('TenYearCHD', axis=1), df['TenYearCHD'])
oversampled_df = pd.concat([pd.DataFrame(oversampled_Y), pd.DataFrame(oversampled_X)], axis=1)

X = oversampled_df.drop(columns =['TenYearCHD'], axis = 1)
y = oversampled_df['TenYearCHD']

y.value_counts().plot.pie(autopct='%.2f', figsize=[5,5])

"""Feature Selection"""

KBest = SelectKBest(score_func=chi2, k='all')
fit = KBest.fit(X, y)
KBest_scores = pd.DataFrame(fit.scores_)
KBest_variables = pd.DataFrame(X.columns)

view = pd.concat([KBest_variables, KBest_scores], axis=1)
view.columns = ['Specs', 'Score']
print(view.nlargest(20, 'Score'))

"""Split Dataset for Training and Testing"""

X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, test_size = 0.2)
print(X_train.shape, X_test.shape)

"""Standardisation"""

scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)
X_test = scaler.fit_transform(X_test)

"""# Modelling"""

def evaluate_model(model, x_test, y_test, prob=True):
    y_pred = model.predict(x_test)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    kappa = cohen_kappa_score(y_test, y_pred)
    confusion = confusion_matrix(y_test, y_pred)

    if prob is False:
        return {'accuracy': accuracy, 'precision': precision,
                'recall': recall, 'f1': f1, 'kappa': kappa, 'confusion': confusion}

    y_prob = model.predict_proba(x_test)[:: , 1]
    fpr, tpr, _ = roc_curve(y_test, y_prob)
    roc_auc = roc_auc_score(y_test, y_prob)
    return {'accuracy': accuracy, 'precision': precision,
            'recall': recall, 'f1': f1, 'kappa': kappa,
            'fpr': fpr, 'tpr': tpr, 'roc_auc': roc_auc, 'confusion': confusion}

"""K Neighbors Classifier"""

knn_model = KNeighborsClassifier(n_neighbors = 2)
knn_fit = knn_model.fit(X_train, y_train)
knn_scores = evaluate_model(knn_model, X_test, y_test)

print('Accuracy: ', knn_scores['accuracy'])
print('Precision: ', knn_scores['precision'])
print('Recall: ', knn_scores['recall'])
print('F1: ', knn_scores['f1'])
print('Kappa: ', knn_scores['kappa'])
print('Area Under Curve: ', knn_scores['roc_auc'])
print('Confusion Matrix:\n', knn_scores['confusion'])

"""Decision Tree Classifier"""

dt_model = DecisionTreeClassifier()
dt_fit = dt_model.fit(X_train, y_train)
dt_scores = evaluate_model(dt_model, X_test, y_test)

print('Accuracy: ', dt_scores['accuracy'])
print('Precision: ', dt_scores['precision'])
print('Recall: ', dt_scores['recall'])
print('F1: ', dt_scores['f1'])
print('Kappa: ', dt_scores['kappa'])
print('Area Under Curve: ', dt_scores['roc_auc'])
print('Confusion Matrix:\n', dt_scores['confusion'])

"""Support Vector Classifier"""

svc_model = SVC()
svc_fit = svc_model.fit(X_train, y_train)
svc_scores = evaluate_model(svc_model, X_test, y_test, False)

print('Accuracy: ', svc_scores['accuracy'])
print('Precision: ', svc_scores['precision'])
print('Recall: ', svc_scores['recall'])
print('F1: ', svc_scores['f1'])
print('Kappa: ', svc_scores['kappa'])
print('Confusion Matrix:\n', svc_scores['confusion'])

"""Logistic Regression Classifier"""

lr_model = LogisticRegression()
lr_fit = lr_model.fit(X_train, y_train)
lr_scores = evaluate_model(lr_model, X_test, y_test)

print('Accuracy: ', lr_scores['accuracy'])
print('Precision: ', lr_scores['precision'])
print('Recall: ', lr_scores['recall'])
print('F1: ', lr_scores['f1'])
print('Kappa: ', lr_scores['kappa'])
print('Area Under Curve: ', lr_scores['roc_auc'])
print('Confusion Matrix:\n', lr_scores['confusion'])

"""Multi-layer Perceptron Classifier"""

mlp_model = MLPClassifier()
mlp_fit = mlp_model.fit(X_train, y_train)
mlp_scores = evaluate_model(mlp_model, X_test, y_test)

print('Accuracy: ', mlp_scores['accuracy'])
print('Precision: ', mlp_scores['precision'])
print('Recall: ', mlp_scores['recall'])
print('F1: ', mlp_scores['f1'])
print('Kappa: ', mlp_scores['kappa'])
print('Area Under Curve: ', mlp_scores['roc_auc'])
print('Confusion Matrix:\n', mlp_scores['confusion'])

"""Random Forest Classifier"""

rfc_model = RandomForestClassifier()
rfc_fit = rfc_model.fit(X_train, y_train)
rfc_scores = evaluate_model(rfc_model, X_test, y_test)

print('Accuracy: ', rfc_scores['accuracy'])
print('Precision: ', rfc_scores['precision'])
print('Recall: ', rfc_scores['recall'])
print('F1: ', rfc_scores['f1'])
print('Kappa: ', rfc_scores['kappa'])
print('Area Under Curve: ', rfc_scores['roc_auc'])
print('Confusion Matrix:\n', rfc_scores['confusion'])

"""Naive Bayes Classifier"""

nbc_model = GaussianNB()
nbc_fit = nbc_model.fit(X_train, y_train)
nbc_scores = evaluate_model(nbc_model, X_test, y_test)

print('Accuracy: ', nbc_scores['accuracy'])
print('Precision: ', nbc_scores['precision'])
print('Recall: ', nbc_scores['recall'])
print('F1: ', nbc_scores['f1'])
print('Kappa: ', nbc_scores['kappa'])
print('Area Under Curve: ', nbc_scores['roc_auc'])
print('Confusion Matrix:\n', nbc_scores['confusion'])

"""XGB Classifier"""

xgb_model = xgb.XGBClassifier(max_depth=20, n_estimators=250)
xgb_fit = xgb_model.fit(X_train, y_train)
xgb_scores = evaluate_model(nbc_model, X_test, y_test)

print('Accuracy: ', xgb_scores['accuracy'])
print('Precision: ', xgb_scores['precision'])
print('Recall: ', xgb_scores['recall'])
print('F1: ', xgb_scores['f1'])
print('Kappa: ', xgb_scores['kappa'])
print('Area Under Curve: ', xgb_scores['roc_auc'])
print('Confusion Matrix:\n', xgb_scores['confusion'])

"""Gradient Boosting Classifier"""

gbc_model = GradientBoostingClassifier(n_estimators=150, max_depth=15, learning_rate=0.4, random_state=44)
gbc_fit = gbc_model.fit(X_train, y_train)
gbc_scores = evaluate_model(gbc_model, X_test, y_test)

print('Accuracy: ', gbc_scores['accuracy'])
print('Precision: ', gbc_scores['precision'])
print('Recall: ', gbc_scores['recall'])
print('F1: ', gbc_scores['f1'])
print('Kappa: ', gbc_scores['kappa'])
print('Area Under Curve: ', gbc_scores['roc_auc'])
print('Confusion Matrix:\n', gbc_scores['confusion'])

"""# Preprocessing 2"""

X2 = oversampled_df.drop(columns =['TenYearCHD', 'sysBP', 'glucose', 'diaBP', 'totChol'], axis = 1)
y2 = oversampled_df['TenYearCHD']

y2.value_counts().plot.pie(autopct='%.2f', figsize=[5,5])

"""Feature Selection"""

KBest = SelectKBest(score_func=chi2, k='all')
fit = KBest.fit(X2, y2)
KBest_scores = pd.DataFrame(fit.scores_)
KBest_variables = pd.DataFrame(X2.columns)

view = pd.concat([KBest_variables, KBest_scores], axis=1)
view.columns = ['Specs', 'Score']
print(view.nlargest(20, 'Score'))

"""Fewer variables"""

X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, shuffle = True, test_size = 0.2)
print(X2_train.shape, X2_test.shape)

"""Standardisation"""

scaler = StandardScaler()

X2_train = scaler.fit_transform(X2_train)
X2_test = scaler.fit_transform(X2_test)

"""# Modelling 2

K Neighbors Classifier
"""

knn_model = KNeighborsClassifier(n_neighbors = 2)
knn_fit = knn_model.fit(X2_train, y2_train)
knn_scores = evaluate_model(knn_model, X2_test, y2_test)

print('Accuracy: ', knn_scores['accuracy'])
print('Precision: ', knn_scores['precision'])
print('Recall: ', knn_scores['recall'])
print('F1: ', knn_scores['f1'])
print('Kappa: ', knn_scores['kappa'])
print('Area Under Curve: ', knn_scores['roc_auc'])
print('Confusion Matrix:\n', knn_scores['confusion'])

"""Decision Tree Classifier"""

dt_model = DecisionTreeClassifier()
dt_fit = dt_model.fit(X2_train, y2_train)
dt_scores = evaluate_model(dt_model, X2_test, y2_test)

print('Accuracy: ', dt_scores['accuracy'])
print('Precision: ', dt_scores['precision'])
print('Recall: ', dt_scores['recall'])
print('F1: ', dt_scores['f1'])
print('Kappa: ', dt_scores['kappa'])
print('Area Under Curve: ', dt_scores['roc_auc'])
print('Confusion Matrix:\n', dt_scores['confusion'])

"""Support Vector Classifier"""

svc_model = SVC()
svc_fit = svc_model.fit(X2_train, y2_train)
svc_scores = evaluate_model(svc_model, X2_test, y2_test, False)

print('Accuracy: ', svc_scores['accuracy'])
print('Precision: ', svc_scores['precision'])
print('Recall: ', svc_scores['recall'])
print('F1: ', svc_scores['f1'])
print('Kappa: ', svc_scores['kappa'])
print('Confusion Matrix:\n', svc_scores['confusion'])

"""Logistic Regression Classifier"""

lr_model = LogisticRegression()
lr_fit = lr_model.fit(X2_train, y2_train)
lr_scores = evaluate_model(lr_model, X2_test, y2_test)

print('Accuracy: ', lr_scores['accuracy'])
print('Precision: ', lr_scores['precision'])
print('Recall: ', lr_scores['recall'])
print('F1: ', lr_scores['f1'])
print('Kappa: ', lr_scores['kappa'])
print('Area Under Curve: ', lr_scores['roc_auc'])
print('Confusion Matrix:\n', lr_scores['confusion'])

"""Multi-layer Perceptron Classifier"""

mlp_model = MLPClassifier()
mlp_fit = mlp_model.fit(X2_train, y2_train)
mlp_scores = evaluate_model(mlp_model, X2_test, y2_test)

print('Accuracy: ', mlp_scores['accuracy'])
print('Precision: ', mlp_scores['precision'])
print('Recall: ', mlp_scores['recall'])
print('F1: ', mlp_scores['f1'])
print('Kappa: ', mlp_scores['kappa'])
print('Area Under Curve: ', mlp_scores['roc_auc'])
print('Confusion Matrix:\n', mlp_scores['confusion'])

"""Random Forest Classifier"""

rfc_model = RandomForestClassifier()
rfc_fit = rfc_model.fit(X2_train, y2_train)
rfc_scores = evaluate_model(rfc_model, X2_test, y2_test)

print('Accuracy: ', rfc_scores['accuracy'])
print('Precision: ', rfc_scores['precision'])
print('Recall: ', rfc_scores['recall'])
print('F1: ', rfc_scores['f1'])
print('Kappa: ', rfc_scores['kappa'])
print('Area Under Curve: ', rfc_scores['roc_auc'])
print('Confusion Matrix:\n', rfc_scores['confusion'])

"""Naive Bayes Classifier"""

nbc_model = GaussianNB()
nbc_fit = nbc_model.fit(X2_train, y2_train)
nbc_scores = evaluate_model(nbc_model, X2_test, y2_test)

print('Accuracy: ', nbc_scores['accuracy'])
print('Precision: ', nbc_scores['precision'])
print('Recall: ', nbc_scores['recall'])
print('F1: ', nbc_scores['f1'])
print('Kappa: ', nbc_scores['kappa'])
print('Area Under Curve: ', nbc_scores['roc_auc'])
print('Confusion Matrix:\n', nbc_scores['confusion'])

"""XGB Classifier"""

xgb_model = xgb.XGBClassifier(max_depth=20, n_estimators=250)
xgb_fit = xgb_model.fit(X2_train, y2_train)
xgb_scores = evaluate_model(nbc_model, X2_test, y2_test)

print('Accuracy: ', xgb_scores['accuracy'])
print('Precision: ', xgb_scores['precision'])
print('Recall: ', xgb_scores['recall'])
print('F1: ', xgb_scores['f1'])
print('Kappa: ', xgb_scores['kappa'])
print('Area Under Curve: ', xgb_scores['roc_auc'])
print('Confusion Matrix:\n', xgb_scores['confusion'])

"""Gradient Boosting Classifier"""

gbc_model = GradientBoostingClassifier(n_estimators=150, max_depth=15, learning_rate=0.4, random_state=44)
gbc_fit = gbc_model.fit(X2_train, y2_train)
gbc_scores = evaluate_model(gbc_model, X2_test, y2_test)

print('Accuracy: ', gbc_scores['accuracy'])
print('Precision: ', gbc_scores['precision'])
print('Recall: ', gbc_scores['recall'])
print('F1: ', gbc_scores['f1'])
print('Kappa: ', gbc_scores['kappa'])
print('Area Under Curve: ', gbc_scores['roc_auc'])
print('Confusion Matrix:\n', gbc_scores['confusion'])